<html>
<link href="597.css" rel="stylesheet" type="text/css">
<body>



<center>
<h2> STA 360/602: Bayesian Methods and Modern Statistics </h2>
</center>
This is a rough schedule for the course and will be updated regularly. Please check this frequently for adjustments. Announcements will be posted here and made in class. It will be up to you to keep up to date on all class announcements and web announcements made for the course. Please read in Hoff before coming to class.
<p>

<p> The course syllabus, labs, and course slides can be found at <a href="https://github.com/resteorts/modern-bayes/blob/master/syllabus/syllabus-sta602-spring18.pdf"> Course materials</a> <p>



<p> Homeworks (based on lecture and lab)  will all be posted on Sakai (and submissions should be done on Sakai as well). Expect about 8--10 assignments for the entire semester. (You lowest homework grade will be dropped). No late homeworks are accepted.  </a>

<hr id="dash">
<a name="lectures"></a>
<h2>Supplementary reading</h2>
I have written both undergraduate and graduate level notes. Please feel free to use these to complement Hoff as needed. Please do watch out for typos!

<h4><a href="books/bayes_manuscripts.pdf"> Some of Bayesian Methods: The Essential Parts (Graduate Level), Author: Rebecca C. Steorts </h2> </a>
Note: Chapter 5 has typos that I have no had time to fix and some parts are not
as clear as I would like. Nevertheless, this should give you some extra examples
and explanations different from Hoff.

<h4><a href="books/babybayes-master.pdf"> Baby Bayes using R, Author: Rebecca C. Steorts </h2> </a>

This material was meant for undergraduate students as a cross-displinary introduction to Bayesian methods, without assuming a knowledge of calculus except that a density integrates to 1. If you're having trouble with Hoff, either as an undergraduate or graduate student, consider reading parts of this. Also, there is
an introduction to probability and statistics (akin with Ch 2 in Hoff). I will assume that you know this. This is all fair game for exams.



<hr id="dash">
<a name="lectures"></a>
<h2>Lecture notes</h2>

<br>
<li>  Module 0: Introduction and Course Expectations
<ul>
  <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-0/00-intro-to-Bayes.pdf">Module 0: Course Expectations</a>,
  <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-0/introToR.html">Module 0: An Intro to R (Go over on your own)</a>
  <li> <a href="modern_bayes17/labs/lab1IntroductiontoR.pdf">Lab 0: An Intro to R</a>
  <li> If you would like a reference text for R, I recommend <a href= "http://shop.oreilly.com/product/9780596809164.do"> The R Cookbook </a>
      <li> To review linear algebra,  I recommend <a href= "https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/"> MIT Linear Algebra Videos </a>
    
</ul>




<br>
<li>  Module 1: An introduction to Bayesian methods
  <ul>
    <li> <a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-1/01-history-of-Bayes.pdf"> Intro to Bayes, Part I </a>,
    <li> <a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-1/01-intro-to-Bayes.pdf"> Intro to Bayes, Part II </a>,      
  <li> Read Ch 1, Ch 2.1 -- 2.6. (Hoff) <br>
    Read Ch 1.1, 2.5--2.7, 2.9  of "Some of Bayesian Methods" <br>
    Read Ch 4 for predictive inference (Hoff). 
</ul>

  
<li>  Module 2: An introduction to Decision Theory
<ul>
    <li> <a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-2/02-intro-to-Bayes.pdf">Module 2 Slides</a>
    <li> Read Read Ch 2.1 -- 2.4 of "Some of Bayesian Methods". This is not covered in Hoff. 
   
</ul>

<br>
<li>  Module 3: An introduction to Normal-Normal Model
<ul>
  <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-3/03-normal-distribution.pdf">Module 3 Slides</a>,
    <li> Read Ch 2, Example 2.7 and 2.8 (in terms of variance derivations) of "Some of Bayesian Methods"
</ul>

<br>
<li>  Module 4: An introduction to Normal-Gamma Model
<ul>
  <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-4/04-normal-gamma.pdf">Module 4 Slides</a>,
    <li> Read Ch 2, Example 2.13 of "Some of Bayesian Methods"
    
</ul>

<br>
<li>  Module 5: Objective (Non-informative or Default Bayes)
<ul>
  <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-5/lecture5-objective.pdf">Module 5 Slides</a>,
  <li> Read Hoff, Chapter 4. <br>
    <li> Read  Chapter 5.1, 5.3 of Some of Bayesian Statistics <br>
Remark: The slides will cover examples not always in Hoff or the notes. 
</ul>

<br>
<li>  Module 6: An introduction to Monte Carlo
<ul>
  <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-6/06-monte-carlo.pdf">Module 6 Slides</a>,
  <li> Read Hoff, Chapter 4. <br>
    <li> Read  Chapter 5.1, 5.3 of Some of Bayesian Statistics <br>
Remark: The slides will cover examples not always in Hoff or the notes. 
</ul>

<br>
<li>  Module 7: An introduction to Metropolis
<ul>
  <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-7/07-metropolis.pdf ">Module 7 Slides -- Metropolis</a>

<li> The reading below  covers the reading for Metroplis and Gibbs sampling.<br>
<li>  Read Hoff, Ch 6 <br>
<li> Read  Chapter 5.2 of "Some of Bayesian Statistics" <br>
<li> For the Metropolis Algorithm, read Hoff 10.2
  </ul>

<br>
<li>  Module 8: An introduction to Gibbs sampling, missing data, and data augmentation
<ul>
  <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-8/08-gibbs.pdf">Module 8 Slides -- Gibbs Sampling</a>,
  <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-8/08-gibbs-multistage.pdf">Module 8 Slides (Part II) -- Gibbs Sampling and Missing Data</a>,
  <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-8/08-data-augment.pdf">Module 8 Slides (Part III) -- Gibbs Sampling and Data Augmentation</a>,
      <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-8/08-data-augment-part2.pdf">Module 8 Slides (Part IV) -- Data Augmentation, The Dirichlet Multinomial, and Mixture Models</a>,
    
 <li> Gibbs reading: You should have already read Ch 6 (Hoff), so review as need be. <br>
<li> Metropolis Hastings: 10.4 and 10.5 (Hoff) <br>
<li> Latent variable allocation: Chapter 12 (Hoff)
</ul>



<br>
<li>  Module 9: The Multivariate Normal Distribution and Missing Data
<ul>
  <li><a href="https://github.com/resteorts/modern-bayes/tree/master/lecturesModernBayes18/lecture-9">Module 9 Slides -- The Multivariate Normal Distribution</a>,
     <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-9/09-missing-data.pdf">Module 9 Slides -- The Multivariate Normal Distribution and Missing Data</a>,
    <li> Read Hoff: Chapter 7.1--7.4
</ul>


<br>
<li>  Module 10: Linear Regression and Probit Regression
<ul>
  <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-10/10-linear-regression.pdf">Module 10 Slides -- Linear Regression</a>,
  <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-10/10-probit-regression.pdf">Module 10 Slides -- Probit  Regression</a>,    
    <li> Read Hoff: Chapter 9
</ul>

<br>

<br>
<li>  Module 11: Model Selection, the g-prior, and model avergaging 
<ul>
  <li><a href="https://github.com/resteorts/modern-bayes/blob/master/lecturesModernBayes18/lecture-11/11-model-selection.pdf"> Module 11 Slides -- Model Selection</a>,    
    <li> Read Hoff: Chapter 12
</ul>

<br>

Additional readings:
<ul>
<li> Credible Intervals): Cred intervals are covered on pages 52 and 267 of Hoff. 
  Read Ch 4.1--4.1 (Cred intervals) in "Some of Bayesian Statistics"
</ul>  


<!--

<br>
<li>  Module 1: An introduction to Bayesian methods, part II
<ul>
  <li><a href="lecturesModernBayes/601-module1-introbayes/lecture2-introToBayes.pdf">Module 1 Slides continued</a>,
     <li><a href="lecturesModernBayes/601-module1-introbayes/findNormalizingConstant.pdf">Deriving the normalizing constant for the Beta-Binomial</a>
    <li><a href="labsModernBayes/lab2IntroductiontoBayes/">Intro to Bayes Lab</a>
</ul>

<br>
<li>  Module 2: An introduction to Decision Theory
<ul>
<li><a href="lecturesModernBayes/601-module2-decision-theory/lecture3-decision-theory.pdf">Module 2 Slides: Decision Theory</a>,
</ul>

<br>
<li>  Module 3: Advanced Bayes
<ul>
  <li><a href="lecturesModernBayes/601-module3-morebayes/lecture4-more-bayes.pdf">Module 3 Slides: Advanced Bayes and Posterior Prediction</a>,
    <li><a href="lecturesModernBayes/601-module3-morebayes/lecture5-more-bayes.pdf">Module 3 Slides: Advanced Bayes and Credible Intervals</a>,
</ul>


<br>
<li>  Module 4: Objective ("Default Bayes")
<ul>
  <li><a href="lecturesModernBayes/601-module4-objective/lecture6-objective.pdf">Module 4 Slides: Objective or Default Bayes</a>,
      <li><a href="lecturesModernBayes/601-module4-objective/lecture7-objective.pdf">Module 4 Slides: A Quick Review of Objective Bayes</a>,

</ul>

<br>
<li> Coverpage for Exam One
<ul>
  <li><a href="examsEdition/midtermFirstExamSpring16/Tasks/showDuringMidtermOne.pdf"> Midterm One Coverpage</a>,
</ul>

<br>
<li>  Module 5: Monte Carlo
<ul>
  <li><a href="lecturesModernBayes/601-module5-montecarlo/lecture8-monte-carlo.pdf">Module 5 Slides: Monte Carlo Methods: Part I: Simple Monte Carlo and Importance Sampling</a>,
      <li><a href="lecturesModernBayes/601-module5-montecarlo/lecture9-monte-carlo.pdf">Module 5 Slides: Monte Carlo Methods: Part II: Rejection Sampling</a>,
</ul>

<br>
<li>  Module 6: Introduction to Markov Chain Monte Carlo
<ul>
  <li><a href="lecturesModernBayes/601-module6-markov/markov-chain-monte-carlo.pdf">Module 6 Slides: MCMC and Metropolis</a>,
</ul>


<br>
<li>  Module 7: Introduction to Gibbs Sampling: Two Stage Gibbs Sampling
<ul>
  <li><a href="lecturesModernBayes/601-module7-onestage/one-stage-gibbs.pdf">Module 7 Slides: Into to Gibbs</a>,
</ul>

<br>
<li>  Module 8: Introduction to Gibbs Sampling: Multi-stage Gibbs Sampling, Missing Data, and Latent Variable Allocation
<ul>
  <li><a href="lecturesModernBayes/601-module8-twostage/two-stage-gibbs.pdf">Module 8 Slides: Multi-stage Gibbs Sampling and Missing Data</a>,
      <li><a href="lecturesModernBayes/601-module8-twostage/data-augmentation.pdf">Module 8 Slides:Latent Variable Augmentation</a>,
</ul>

<br>
<li>  Module 9: Metropolis Hastings
<ul>
  <li><a href="lecturesModernBayes/601-module9-metropolis/metropolis.pdf">Module 9 Slides: Metropolis Hastings</a>,
</ul>

<br>
<li>  Module 10: Multivariate Methods
<ul>
  <li><a href="lecturesModernBayes/601-module10-multivariate-normal/multivariate-normal.pdf">Module 10 Slides: The Multivariate Normal</a>,
  <li><a href="lecturesModernBayes/601-module10-multivariate-normal/dirichlet-multinomial.pdf">Module 10 Slides:  Multivariate Distributions: Dirichlet-Multinomial</a>,
       <li><a href="lecturesModernBayes/601-module10-multivariate-normal/linear-regression.pdf">Module 10 Slides: Linear Regression</a>,
</ul>

<br>
<li>  Module 11: An Intro to Bayesian Nonparametrics
<ul>
  <li><a href="lecturesModernBayes/601-module12-BNP/introToBNP.pdf">Module 11 Slides: An Intro to Bayesian Nonparametrics </a>,
</ul>

<br>
<li> Last Lecture: Exam Format and Course Evaluations
<ul>
  <li><a href="lecturesModernBayes/601-module-recap/finalLecture.pdf">Final Exam Format and Course Evaluations </a>,
</ul>



-->


<br>
<a href="#top">Top</a>
